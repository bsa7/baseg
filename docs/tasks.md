# Другие задачи

## Информация о паркет файле
Это временный раздел, в котором находится инструкция по работе со скриптом,
реализующим интеграцию с Apache Spark.

Для запуска скрипта:
1. Запустить докер контейнер со спарком (или вообще все докер контейнеры):
  ```bash
  docker-compose up
  ```

2. Запустить скрипт:
  ```bash
  python ./tasks/parquet_info_with_spark.py ../baseg-shared/data/wallet_urfu.parquet.gzip
  ```

  в результате работы скрипта вы должны увидеть на экране следующее:
  ```bash
  =============== Файл ../baseg-shared/data/wallet_urfu.parquet.gzip ===============
  ...
  +-------+----------+--------------------+
  |partner|  rep_date|            monetary|
  +-------+----------+--------------------+
  | 468726|2016-01-02|2.673749485266116...|
  | 134145|2016-01-02|1.069499794106446...|
  |  66774|2016-01-02|2.22857565260091E-12|
  | 178928|2016-01-02|8.730610564134256...|
  | 679693|2016-01-02|3.492244225653702...|
  ...
  ```

  Это означает, что спарк инициализирован и работоспособен.

3. Так же скрипт можно запустить в докер-контейнере. Для этого нужно, чтобы на одном уровне с папкой проекта
была создана папка baseg-shared, в которой будут находится данные, доступные из докер контейнера
Структура папок должна быть такой:

```bash
+ -- baseg (Папка с проектом)
+ -- baseg-shared
  + -- data
    + -- wallet_urfu.parquet.gzip
```

После этого можно запустить Python скрипт следующим образом:
```bash
./docker/run "python -m tasks.parquet_info input_file=../data/wallet_urfu.parquet.gzip"
```
